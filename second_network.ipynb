{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Loading in and dividing data\n",
    "# reads in data\n",
    "images = np.load(\"images.npy\")\n",
    "pv_out = np.load(\"pv_outputs.npy\")\n",
    "\n",
    "# randomly divides into a training set and a validation set\n",
    "num_imgs = images.shape[0]\n",
    "indices = np.arange(num_imgs)\n",
    "np.random.shuffle(indices)\n",
    "X_train, X_val = images[indices[:int(0.8 * num_imgs)]], images[indices[int(0.8 * num_imgs):]]\n",
    "y_train, y_val = pv_out[indices[:int(0.8 * num_imgs)]], pv_out[indices[int(0.8 * num_imgs):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "def cnn_73_model(X, y, is_training):\n",
    "    # CBP sandwich 1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=X,\n",
    "        filters=32,\n",
    "        kernel_size=[7, 7],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    bn1 = tf.layers.batch_normalization(inputs=conv1, axis=1)\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=bn1,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    bn2 = tf.layers.batch_normalization(inputs=conv2, axis=1)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=bn2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # CBP sandwich 2\n",
    "    conv3 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[3, 3],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    bn3 = tf.layers.batch_normalization(inputs=conv3, axis=1)\n",
    "    conv4 = tf.layers.conv2d(\n",
    "        inputs=bn3,\n",
    "        filters=64,\n",
    "        kernel_size=[3, 3],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    bn4 = tf.layers.batch_normalization(inputs=conv4, axis=1)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=bn4, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Two fully connected nets\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 15 * 15 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=dense, rate=0.4, training=is_training)\n",
    "    regression = tf.layers.dense(inputs=dropout, units=1)\n",
    "    regression = tf.reshape(regression, [-1])\n",
    "    return regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model \n",
    "def run_model(session, pred_y_var, loss_var,\n",
    "              x_var,y_var,is_training, Xd, yd,\n",
    "              epoch_idx=0, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    rel_err_var = tf.divide(tf.abs(tf.subtract(y_var, pred_y_var)), y_var)\n",
    "    accuracy = tf.reduce_mean(rel_err_var)\n",
    "\n",
    "    # shuffle indices\n",
    "    train_indices = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indices)\n",
    "    training_now = training is not None\n",
    "\n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [loss_var, rel_err_var, accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "\n",
    "    # counter\n",
    "    iter_cnt = 0\n",
    "\n",
    "    # keep track of losses and accuracy\n",
    "    errors = 0\n",
    "    losses = []\n",
    "\n",
    "    # make sure we iterate over the dataset once\n",
    "    for i in range(int(Xd.shape[0] / batch_size)+1):\n",
    "        # generate indices for the batch\n",
    "        start_idx = (i * batch_size) % Xd.shape[0]\n",
    "        idx = train_indices[start_idx:start_idx + batch_size]\n",
    "\n",
    "        # create a feed dictionary for this batch\n",
    "        feed_dict = {x_var: Xd[idx, :],\n",
    "                     y_var: yd[idx],\n",
    "                     is_training: training_now}\n",
    "        # get batch size\n",
    "        actual_batch_size = yd[i:i + batch_size].shape[0]\n",
    "\n",
    "        # have tensorflow compute loss and correct predictions\n",
    "        # and (if given) perform a training step\n",
    "        loss, rel_err, _ = session.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "        # aggregate performance stats\n",
    "        losses.append(loss * actual_batch_size)\n",
    "        errors += np.sum(rel_err)\n",
    "\n",
    "        # print every now and then\n",
    "        if training_now and (iter_cnt % print_every) == 0:\n",
    "            print(\"Iteration {0}: with minibatch training loss = {1:.3g} and relative error of {2:.2g}\"\n",
    "                  .format(iter_cnt, loss, np.sum(rel_err) / actual_batch_size))\n",
    "        iter_cnt += 1\n",
    "    total_error = errors / Xd.shape[0]\n",
    "    total_loss = np.sum(losses) / Xd.shape[0]\n",
    "\n",
    "    print(\"Epoch {2}, Overall loss = {0:.3g} and relative error of {1:.3g}\"\n",
    "          .format(total_loss, total_error, epoch_idx+1 ))\n",
    "\n",
    "    if plot_losses:\n",
    "        plt.plot(losses)\n",
    "        plt.grid(True)\n",
    "        plt.title('Epoch {} Loss'.format(epoch_idx + 1))\n",
    "        plt.xlabel('minibatch number')\n",
    "        plt.ylabel('minibatch loss')\n",
    "        plt.show()\n",
    "\n",
    "    return total_loss, total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build computational graph\n",
    "tf.reset_default_graph()  # Reset computational graph\n",
    "\n",
    "x_var = tf.placeholder(tf.float32, [None, 60, 60, 3]) # x variable\n",
    "y_var = tf.placeholder(tf.float32, [None]) # y variable\n",
    "is_training = tf.placeholder(tf.bool) # flag\n",
    "pred_y_var = cnn_73_model(x_var, y_var, is_training) # model in use\n",
    "mean_loss = tf.losses.mean_squared_error(y_var, pred_y_var) # loss in use\n",
    "\n",
    "# Define optimizer and optimize session parameter\n",
    "# define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: with minibatch training loss = 3.66e+06 and relative error of 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20: with minibatch training loss = 3.28e+06 and relative error of 5.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40: with minibatch training loss = 9.96e+05 and relative error of 2.3\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# initialize all variable\n",
    "num_epochs = 30\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# initialize loss and rel_err history list\n",
    "train_loss_hist = []\n",
    "train_error_hist = []\n",
    "val_error_hist = []\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    print('Training')\n",
    "    train_loss, train_error = run_model(sess, pred_y_var, mean_loss,\n",
    "                                        x_var,y_var,is_training,\n",
    "                                        X_train, y_train, epoch_idx, 128, 20, train_step,False)\n",
    "    print('Validation')\n",
    "    val_loss, val_error = run_model(sess, pred_y_var, mean_loss,\n",
    "                                    x_var, y_var, is_training, X_val, y_val)\n",
    "\n",
    "    train_loss_hist.append(train_loss)\n",
    "    train_error_hist.append(train_error)\n",
    "    val_error_hist.append(val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting final results\n",
    "# plot training loss history\n",
    "plt.plot(train_loss_hist)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('training loss')\n",
    "plt.savefig('training_loss_history.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot relative error history\n",
    "plt.plot(train_error_hist[7:], label='training relative error')\n",
    "plt.plot(val_error_hist[7:], label='validation relative error')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('relative error')\n",
    "plt.legend()\n",
    "plt.savefig('relative_error.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}